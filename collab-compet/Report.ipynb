{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning is part of the machine learning.Reinforcement Learning has two major elements environment and agent.Agent interact with environment and recives signal depending how it interact with environment.Using the signal(reward) agent can learn some behaviour about environment.Environment provide state (observation) to agent to take appropriate action and in in return it receives regard from environment.\n",
    "\n",
    "Deep Reinforement Learning uses the neural networks in agent as function approximator to predict actions directly or uses it contruct value function using which it can decide appropriate action.DDPG is based on Actor-Critic method.Actor predict the action depending on the state and action are deterministic and critic is used to predict the q value, which say how good is the action given the state.DDPG is off-policy algo. This similar to DQN,uses replay buffer and target network to train.Replay buffer help it to decolerate the sample from different batches and target network is used so the next state q value and next state action are stable, this network are updated slowly.MADDPG is algorithm used to train multiple agent.using indivisual agent makes the envirnoment non statinonary which causes the algorithm not to converge.So in MADDPG the actor is only given information of observation while the critic is given state information of the envirnoment.Critic will use the full state information to stablise the actor and also each agent can have their own reward function which make agents to tkae compititive or colabration approch. This project uses the distribution approch to calculate the the q value similar to c51 algorithm. \n",
    "\n",
    "Choosing action during training can be trick due to knowledge regarding the environment(we have narrow understanding about the environment) this is offen called exploration vs exploitation.We add noise to action to have better exploration during training.\n",
    "\n",
    "The project consists of three files model.py,ddpg_agent.py and control.ipynb.Model.py define the neural network used to predict the z distribution value and action for given state.The Actor class predict the deterministic action for given state. it has three layers with neurons 400,300,action_dim, relu is used as the activation function.Critic is also a neural network with three layers with 400,300 plus action_dim,z distribution and relu is used as activation function.Xavier initilization for top layer and last layer is initilized uniformly between +-3e-3.\n",
    "\n",
    "Agent file contains the reaply buffer,ddpg agent class and the maddpg agent class.Replay buffer hold the sample of previous transitions. Size of replay buffer is set to 1e5. when we sample a batch of 256 samples from replay buffer.gaussian noise is used to petrub the action during acting policy to explore the environment and noise is scaled as the training progress.agent class contains neural networks for local and target network for both actor and critic along with their optimizers.both optimizers are learing rate is set to 1e-4, weight_decay for critic is set to 0 and for actor it is set to 0.Soft update is performed between the local and target network, taking 1e-3 of local network and updating the target network.\n",
    "\n",
    "For training the agent we have 2 agent, each control 1 racket and single replay buffer to store the experiences.For every  step of episode the agents are trained for 3 times after 300 episodes.As ddpg is off policy algorithm i am using single replay buffer and for z distribution parameters i have choosen the vmin and vmax as +-1 respectively.51 nodes to approximate 51 q values between +-1.ddpg agent critic provides the probility for all the q values between the +-1 which are then multplied with gamma and added with reward.the ditribustion is shifted to get back the distribution to original domain we find the nearest two nodes and adjust corresponding probalities.then kl diverges between the target and local distribution is used to reduce the distribution mismatch.\n",
    "\n",
    "Below is the plot of the avgerage score vs episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plot.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Other alogrithm that could be implemented is self play to solve the environment similar to alpha zero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

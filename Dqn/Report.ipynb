{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                      Duelling Double Deep Q network\n",
    "    \n",
    "    Reinforcement learning is part of the machine learning.Reinforcement Learning has two major elements \n",
    "    environment and agent.Agent interact with environment and recives signal depending how it interact with \n",
    "    environment.Using the signal(reward) agent can learn some behaviour about environment.Environment provide state \n",
    "    (observation) to agent to take appropriate action and in in return it receives regard from environment.\n",
    "    \n",
    "    Deep Reinforement Learning uses the neural networks in agent as function approximator to predict actions directly \n",
    "    or uses it contruct value function using which it can decide appropriate action.Deep Q network algorithm estimate \n",
    "    the q value for the given state(observation) and using this value it can take decision to maximise overall reward \n",
    "    it can receive.\n",
    "    \n",
    "    Dqn is part value iteration algorithm where value function or action value function is used to take decision.\n",
    "    First the algorithm learn to estimate the value function wrt to state and then it can create policy which \n",
    "    maximizes reward for every state q network Work well in sovling when state space is less we cam use dynamic \n",
    "    programming to solve environment.when we use neural network as function approximator it can become hard to \n",
    "    converge to solution.This can be solved by using  methods like replay buffer and fixed q target.Neural network \n",
    "    have tendency to learn the temporal changes to solve this, we can use buffer which holds the perivous experinces \n",
    "    and sample experince along with previous experiences.In Dqn we approximate the q value for current state and \n",
    "    estimate value is reward plus q value from next state from same network.In this case it hard for network to \n",
    "    converge as the value it predict moves,so we you use fixed target network.Secondary network similar to primary\n",
    "    network is used to extimate the q value for next state and this network is updated slowly so that target value \n",
    "    doesn't move quickly.\n",
    "    \n",
    "    Choosing action during training can be trick due to knowledge regarding the environment(if have narrow \n",
    "    understanding about the environment) this is offen called exploration vs exploitation.We use concept called \n",
    "    epsilon greedy method to choose action, we assign high probabilty to action which gave max q value and some \n",
    "    epsilon amount of probablity distributed among all action.Initally Epsilon is keep high where exploration is\n",
    "    given prefernce and when our network is trained and network is able to predict Q value appropriately we can \n",
    "    decrease epsilon where we are exploiting the environment behaviour to get high reward.\n",
    "    \n",
    "    The project consists of three files model.py,agent.py and navigation.ipynb .Model.py define the neural network \n",
    "    used to predict the q value for given state for all action.Model is a deep neural network with three layer having\n",
    "    512,128,64 hidden neurons.Input to the network is state which is vector of 37 dimension and output is q value for\n",
    "    all the actions in this case 4.Network predict value value function and advantage function using this we can \n",
    "    calculate q value that is q(s,a) = v(s) + A(s,a).\n",
    "    \n",
    "    Agent.py is file containing replay buffer and agent which learn to solve network.Replay buffer contains function\n",
    "    to add recent experince(state,action,reward,next state and done) to Deque of size given by max_size.Sample \n",
    "    function provides the batch size of samples from previous experince.Agent class create agent which learn to solve \n",
    "    the rl problem.Learn function is used to train neural network to predict q value given state.Act function is used\n",
    "    to take action given state using e-epsilon greedy function.Update function is used to update the weight of target \n",
    "    network with some part of local network(fixed target network).Step function is used to interact with env and \n",
    "    train the local network for every four interaction with the env given we have enough samples to train network.iy \n",
    "    uses adam optimizer to optimize the neural network with learning rate set as 5e-4.During training we take batch \n",
    "    of 64 sample of experience and train double dqn.\n",
    "    \n",
    "    This project is duelling double dqn which is imporvement to dqn.When network predicts the q value it is over \n",
    "    optimistic and to reduce the noise in estimation of q value we use local network to get action with max q value.\n",
    "    This action is used in target network to get the q value which reduces the noise as both the network needs to\n",
    "    agree with action to give max Q value.Duelling network predict value function and advantage function of choosing \n",
    "    the action over other action.In Some cases taking any action doesn't change the outcome and some cases action \n",
    "    matter where advantage function comes in to play.In determenistic policy Q*(s,a) = V(s) a is choosen to be armax \n",
    "    Qvalue.Hence the Advantage function will be zero.in practice its hard to know Q* = V* as q is defined as \n",
    "    v(s)+a(s,a). to avoid this we subtract the max of ardvantage function in practice we subtarct the mean of the \n",
    "    advantage function.\n",
    "    \n",
    "    \n",
    "    Nagvigaion.ipynb has the cell to import library,setup the env,and initialze the agent.Def dqn function train the \n",
    "    agent to solve the env.the function takes in max number of episode,number of step in each episode and variable \n",
    "    required to decay e used in e-greedy algo.we rest the env at the start of the episode collect the state and \n",
    "    interact with env for max steps or when env gives done as true.for every four step in env we train our network\n",
    "    once.when the average score for for hundered episode is greater than a certain value agent is able to solve the\n",
    "    env and we store the network weight.\n",
    "    \n",
    "    \n",
    "    The agent performance can be furture improved using priotized replay buffer where the sample are taken according\n",
    "    to the error and since sampling is not random we need to take care by weighting them during training.Other things \n",
    "    like network input can be multiple of state in temporal domain instead of one state in temporal domain this will\n",
    "    help the agent to understand the movement of things.we can also use model based reinforment learning to solve the \n",
    "    env. \n",
    "    \n",
    "    My agent takes around 600 episode to get score of 13+ but i have kept the threshold of 15.5 to solve the env\n",
    "    which takes around 1000 steps.Bleow is the plot of the score vs episode\n",
    "\n",
    "<img src=\"plot.png\">\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

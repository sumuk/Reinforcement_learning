{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning is part of the machine learning.Reinforcement Learning has two major elements\n",
    "environment and agent.Agent interact with environment and recives signal depending how it interact with \n",
    "environment.Using the signal(reward) agent can learn some behaviour about environment.Environment provide state \n",
    "(observation) to agent to take appropriate action and in in return it receives regard from environment.\n",
    "\n",
    "Deep Reinforement Learning uses the neural networks in agent as function approximator to predict actions directly \n",
    "or uses it contruct value function using which it can decide appropriate action.DDPG is based on Actor-Critic method.Actor predict the action depending on the state and action are deterministic and critic is used to predict the q value, which say how good is the action given the state.DDPG is off-policy algo. This similar to DQN,uses replay buffer and target network to train.Replay buffer help it to decolerate the sample from different batches and target network is used so the next state q value and next state action are stable, this network are updated slowly.\n",
    "\n",
    "Choosing action during training can be trick due to knowledge regarding the environment(we have narrow \n",
    "understanding about the environment) this is offen called exploration vs exploitation.We add noise to action to have better exploration during training.\n",
    "\n",
    "The project consists of three files model.py,sumuk_ddpg_agent.py and control.ipynb.Model.py define the neural network used to predict the q value and action for given state.The Actor class predict the deterministic action for given state. it has three layers with neurons 400,300,action_dim, batch normalization after first linear layer and relu is used as the activation function.Critic is also a neural network with three layers with 400,300 plus action_dim,1 with batch normalization after first linear layer and relu is used as activation function.Xavier initilization for top layer and last layer is initilized uniformly between +-3e-3.\n",
    "\n",
    "Agent file contains the reaply buffer,noise class and the agent class.Replay buffer hold the sample of previous transitions. Size of replay buffer is set to 1e6. when we sample a batch of 1024 samples from replay buffer.noise class is used to petrub the action during acting policy to explore the environment.agent class contains neural networks for local and target network for both actor and critic along with their optimizers.both optimizers are learing rate is set to 1e-3, weight_decay for critic is set to 0 and for actor it is default.Soft update is performed between the local and target network, taking 1e-3 of local network and updating the target network.\n",
    "\n",
    "For training the agent we have 4 agent, each control 5 arms and single replay buffer to store the experiences.For every 20 steps of episode the agents are trained for 10 times.As ddpg is off policy algorithm i am using single replay buffer and 4 diffenrent policy should visit different state which help other in learning.Agents get an average of men score of 30 within 24 episodes.\n",
    "\n",
    "Below is the plot of the avgerage score vs episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm can be improved to prioritised replay buffer and n step bootstrapping instead of 1 step return.\n",
    "Other alogrithm can be implemented are  D4PG,PPO or A3C to solve the environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
